# SwiftIndex Configuration
# Generated by 'swiftindex init'

[embedding]
# Provider options: mlx, swift, ollama, voyage, openai
provider = "openai"
model = "text-embedding-3-small"
dimension = 1536

# Provider examples and defaults:
# MLX (Apple Silicon + Metal toolchain)
# provider = "mlx"
# model = "mlx-community/Qwen3-Embedding-0.6B-4bit-DWQ"
# Swift Embeddings (CPU)
# provider = "swift"
# model = "all-MiniLM-L6-v2"
# Ollama (local server)
# provider = "ollama"
# model = "nomic-embed-text"
# Voyage (cloud)
# provider = "voyage"
# model = "voyage-large-2"
# OpenAI (cloud)
# provider = "openai"
# model = "text-embedding-3-large"
# Gemini (Google AI)
# provider = "gemini"
# model = "text-embedding-004"

[search]
semantic_weight = 0.7
rrf_k = 60
multi_hop_enabled = true
multi_hop_depth = 2
output_format = "toon"  # toon (token-optimized), human, or json

# Search defaults (can be overridden via CLI flags)
limit = 20                      # default --limit
expand_query_by_default = false # default --expand-query
synthesize_by_default = false   # default --synthesize
# default_extensions = ["swift", "ts"]  # default --extensions (empty = all)
# default_path_filter = "Sources/**"     # default --path-filter (glob syntax)

# LLM-powered search enhancements (query expansion, result synthesis)
# Requires LLM provider access (mlx, claude-code-cli, codex-cli, ollama, or openai)
[search.enhancement]
enabled = true

# Utility tier - fast operations (query expansion, follow-ups)
[search.enhancement.utility]
provider = "openai"  # mlx | claude-code-cli | codex-cli | ollama | openai
model = "gpt-4o-mini"  # fast and cheap
timeout = 60

# Synthesis tier - deep analysis (result summarization)
[search.enhancement.synthesis]
provider = "openai"  # mlx | claude-code-cli | codex-cli | ollama | openai
model = "gpt-4o-mini"  # fast and cheap
timeout = 120

# Provider examples:
# mlx: Uses MLX for local text generation (Apple Silicon only, fully offline)
# claude-code-cli: Uses 'claude' CLI (requires: npm install -g @anthropic-ai/claude-code)
# codex-cli: Uses 'codex' CLI (requires: npm install -g @openai/codex)
# ollama: Uses local Ollama server (requires: ollama serve)
# openai: Uses OpenAI API (requires: OPENAI_API_KEY env var)

[indexing]
exclude = [".git", ".build", "DerivedData"]
include_extensions = [".swift", ".m", ".h"]
max_file_size = 1000000
chunk_size = 1500
chunk_overlap = 200
# max_concurrent_tasks defaults to CPU count

[storage]
index_path = ".swiftindex"
# cache_path = "~/.cache/swiftindex"

[watch]
debounce_ms = 500

[logging]
level = "info"
